---
title: "Digit Recognizer"
output: html_notebook
---


1. Load and explore the data
```{r}

train <- read.csv("train.csv")
test <- read.csv("test.csv")

dim(train)
```

```{r}
table(as.factor(train$label))
```

```{r}
install.packages("ggplot2")
library(ggplot2)

ggplot(train, aes(x = as.factor(label), fill=label)) + geom_bar(stat = "count", color = "white") + scale_fill_gradient(low = "pink", high = "blue", guide = F) + labs(x = "labels", main = "digit count in training data")

```

```{r}


sample <- sample(1:nrow(train),50)
var <- t(train[sample,-1])

var_matrix <- lapply(1:50,function(x) matrix(var[,x],ncol=28))
head(var_matrix)
opar <- par(no.readonly = T)
par(mfrow=c(5,10),mar=c(.1,.1,.1,.1))

for(i in 1:50) {
  for(j in 1:28) {
    var_matrix[[i]][j,] <- rev(var_matrix[[i]][j,])
  }
  image(var_matrix[[i]],col=grey.colors(225),axes=F)
}

```


2. Data Preprocessing

Removing the 208 near zero variance predictors

```{r}

install.packages("caret")
library(caret)

nzero <- nearZeroVar(train[,-1], freqCut = 10000/1, uniqueCut = 1/7, saveMetrics = T)
head(nzero)
sum(nzero$zeroVar)

sum(nzero$nzv)
nzero$nzv

omitvar <- rownames(nzero[nzero$nzv==T,])
vardata <- setdiff(names(train),omitvar)
train <- train[,vardata]
dim(train)
```

Principal Component Analysis

```{r}

#Scaling tarining data to maximum
label <- as.factor(train[[1]])
train$label <- NULL
train <- train/255


#obtaining covariance matrix of the predictors

covtrain <- cov(train)

```

Applying PCA to covariance matrix
```{r}
train_pc <- prcomp(covtrain)
varex <- train_pc$sdev^2/sum(train_pc$sdev^2)
varcum <- cumsum(varex)
result <- data.frame(num=1:length(train_pc$sdev),
                         ex=varex,
                         cum=varcum)

plot(result$num,result$cum,type="b",xlim=c(0,100),
     main="Variance Explained by Top 100 Components",
     xlab="Number of Components",ylab="Variance Explained")
abline(v=25,lty=2)
```

Clearly, 25 is a good choice, so we will utilize 25 components to fit model. Here, we can plot first principal components.

```{r}
train_score <- as.matrix(train) %*% train_pc$rotation[,1:25]
train <- cbind(label,as.data.frame(train_score))

colors <- rainbow(length(unique(train$label)))
names(colors) <- unique(train$label)
plot(train$PC1,train$PC2,type="n",main="First Two Principal Components")
text(train$PC1,train$PC2,label=train$label,col=colors[train$label])
```

Train and Prediction
Well, it???s time to train the data and get a proper model. we will rely on the svm classification algorithm. We then get the new test data with the process like train data. Finally, we predict the new test data with svm model. 


```{r}
svm_mdl <- train(label~.,data=train,
                 method="svmRadial",
                 trControl=trainControl(method="cv",
                                        number=5),
                 tuneGrid=data.frame(sigma = 0.01104614,
                                      C = 3.5))
svm_mdl



test <- test[,var[-1]]/255
test <- as.matrix(test) %*% train_pc$rotation[,1:25]
test <- as.data.frame(test)

pred <- predict(svm_mdl$finalModel,test,type="response")
prediction <- data.frame(ImageId=1:nrow(test),Label=pred)
                         
```
















